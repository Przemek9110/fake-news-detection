{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3ad265",
   "metadata": {},
   "source": [
    "# **Fake News Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103cfc2",
   "metadata": {},
   "source": [
    "# **Project Description**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693c0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d9e839b",
   "metadata": {},
   "source": [
    "# **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0550c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_fake = pd.read_csv('data/WELFake_Dataset.csv', index_col = 0)\n",
    "fake_news_net = pd.read_csv('data/FakeNewsNet.csv', index_col=False)\n",
    "true = pd.read_csv('data/true.csv', index_col=False)\n",
    "fake = pd.read_csv('data/fake.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37378c41",
   "metadata": {},
   "source": [
    "# **2. Data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a538591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_plot(df, label, plot_name):\n",
    "    df.groupby(label)[label].count().plot(kind='pie', autopct='%1.1f%%', title=plot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a65959",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "def word_cloud_plot(df, groupby, agg_column):\n",
    "    wc = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(df.groupby(groupby)[agg_column].sum()[0])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57dd52",
   "metadata": {},
   "source": [
    "## **2.1 WEL Fake dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be268c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92631585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProfileReport(wel_fake, title='WELFake').to_file('WELFake.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0194ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = wel_fake.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_fake.fillna(\" \", inplace=True)\n",
    "wel_fake.dropna(inplace=True)\n",
    "wel_fake = wel_fake.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c316ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "wel_fake['label'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fe5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_plot(wel_fake,'label', 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0691a",
   "metadata": {},
   "source": [
    "## **2.2 Fake news net dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5cb37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_news_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d1913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='real', data=fake_news_net)\n",
    "plt.title('Distribution of Real and Fake News')\n",
    "plt.xlabel('News Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Fake', 'Real'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe615c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(fake_news_net[fake_news_net['real'] == 1]['tweet_num'], bins=30, color='blue', label='Real')\n",
    "sns.histplot(fake_news_net[fake_news_net['real'] == 0]['tweet_num'], bins=30, color='red', label='Fake')\n",
    "plt.title('Distribution of Tweet Numbers for Real and Fake News')\n",
    "plt.xlabel('Number of Tweets')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 source domains for real and fake news\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "fake_news_net[fake_news_net['real'] == 1]['source_domain'].value_counts().head(10).plot(kind='barh', color='blue')\n",
    "plt.title('Top 10 Source Domains for Real News')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Source Domain')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "fake_news_net[fake_news_net['real'] == 0]['source_domain'].value_counts().head(10).plot(kind='barh', color='red')\n",
    "plt.title('Top 10 Source Domains for Fake News')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Source Domain')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0795ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_net.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c87eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProfileReport(fake_news_net, title='FakeNewsNet').to_file('FakeNewsNet.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_net = fake_news_net.dropna()\n",
    "\n",
    "print('Shape after removing missing values:', fake_news_net.shape)\n",
    "\n",
    "real_news = fake_news_net[fake_news_net['real'] == 1]\n",
    "fake_news = fake_news_net[fake_news_net['real'] == 0]\n",
    "\n",
    "real_news_downsampled = real_news.sample(len(fake_news), random_state=1)\n",
    "\n",
    "fake_news_net_balanced = pd.concat([real_news_downsampled, fake_news])\n",
    "\n",
    "print('Shape of balanced dataset:', fake_news_net_balanced.shape)\n",
    "\n",
    "sns.countplot(x='real', data=fake_news_net_balanced)\n",
    "plt.title('Distribution of Real and Fake News in the Balanced Dataset')\n",
    "plt.xlabel('News Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Fake', 'Real'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_plot(fake_news_net,'real', 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74197586",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_net = fake_news_net_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacee0d",
   "metadata": {},
   "source": [
    "## **2.3 Fake and True dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef45822",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake['label'] = 0\n",
    "\n",
    "true['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_true = pd.concat([fake, true]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86022ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProfileReport(fake_true, title='FakeTrue').to_file('FakeTrue.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c04c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_true.groupby(['subject', 'label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_true.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9485e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "balance_plot(fake_true,'label', 'Fake True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_true['text'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_plot(fake_true,'label', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe03ce0",
   "metadata": {},
   "source": [
    "# **3. Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66709e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    \n",
    "    if preprocessed_text == \"\":\n",
    "        preprocessed_text = \"placeholder\"\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714b302",
   "metadata": {},
   "source": [
    "## **3.1 WEL Fake preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b5723",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wel_fake['text'] = wel_fake['text'].apply(preprocess_text)\n",
    "wel_fake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c442a5c",
   "metadata": {},
   "source": [
    "### **3.1.2 Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ca5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2, max_features=1000)\n",
    "\n",
    "bow = vectorizer.fit_transform(wel_fake['text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_bow = pd.DataFrame(bow.toarray(), columns=feature_names)\n",
    "\n",
    "X_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06dcc32",
   "metadata": {},
   "source": [
    "### **Split Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wel_fake['label']\n",
    "X_train_bow_wel, X_test_bow_wel, y_train_bow_wel, y_test_bow_wel = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "X_train_bow_wel, X_val_bow_wel, y_train_bow_wel, y_val_bow_wel = train_test_split(X_train_bow_wel, y_train_bow_wel, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1269e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bow_wel.value_counts(normalize=True) #imbalance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1bc4b9",
   "metadata": {},
   "source": [
    "### **3.1.3 Word 2 Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd777743",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = wel_fake['text'].apply(lambda x: x.split())\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b532811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_train_test_prepatarion(data):\n",
    "    X_train_vec = [] \n",
    "    for sentence in data:\n",
    "        sentence_vec = []\n",
    "        for word in sentence.split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                sentence_vec.append(model.wv[word])\n",
    "            else:\n",
    "                sentence_vec.append(np.zeros(100)) #100 equal to vector_size in w2v_model\n",
    "        X_train_vec.append(np.mean(sentence_vec, axis=0))\n",
    "    return X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_wel = w2v_train_test_prepatarion(wel_fake['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24de9a",
   "metadata": {},
   "source": [
    "### **Split Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wel_fake['label']\n",
    "X_train_w2v_wel, X_test_w2v_wel, y_train_w2v_wel, y_test_w2v_wel = train_test_split(X_w2v_wel, y, test_size=0.2, random_state=42)\n",
    "X_train_w2v_wel, X_val_w2v_wel, y_train_w2v_wel, y_val_w2v_wel = train_test_split(X_train_w2v_wel, y_train_w2v_wel, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_w2v_wel.value_counts(normalize=True) #imbalance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a1683",
   "metadata": {},
   "source": [
    "## **3.2 Fake News Net preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_net['title'] = fake_news_net['title'].apply(preprocess_text)\n",
    "fake_news_net.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39545f",
   "metadata": {},
   "source": [
    "### 3.2.1 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f952efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2, max_features=1000)\n",
    "\n",
    "bow = vectorizer.fit_transform(fake_news_net['title'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_bow = pd.DataFrame(bow.toarray(), columns=feature_names)\n",
    "\n",
    "X_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fake_news_net['real']\n",
    "X_train_bow_net, X_test_bow_net, y_train_bow_net, y_test_bow_net = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "X_train_bow_net, X_val_bow_net, y_train_bow_net, y_val_bow_net = train_test_split(X_train_bow_net, y_train_bow_net, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bed2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bow_net.value_counts(normalize=True) #imbalance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e5069",
   "metadata": {},
   "source": [
    "### **3.2.2 Bag of Word 2 Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647490eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = fake_news_net['title'].apply(lambda x: x.split())\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cba5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_net = w2v_train_test_prepatarion(fake_news_net['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547895f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fake_news_net['real']\n",
    "X_train_w2v_net, X_test_w2v_net, y_train_w2v_net, y_test_w2v_net = train_test_split(X_w2v_net, y, test_size=0.2, random_state=42)\n",
    "X_train_w2v_net, X_val_w2v_net, y_train_w2v_net, y_val_w2v_net = train_test_split(X_train_w2v_net, y_train_w2v_net, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d405ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_w2v_net.value_counts(normalize=True) #imbalance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9783b",
   "metadata": {},
   "source": [
    "## **3.3 Fake and True preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_true.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_true.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4994de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_true['text'] = fake_true['text'].apply(preprocess_text)\n",
    "fake_true.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a0e8e",
   "metadata": {},
   "source": [
    "### **3.3.1 Bag of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2, max_features=1000)\n",
    "\n",
    "bow = vectorizer.fit_transform(fake_true['text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_bow = pd.DataFrame(bow.toarray(), columns=feature_names)\n",
    "\n",
    "X_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fake_true['label']\n",
    "X_train_bow_ft, X_test_bow_ft, y_train_bow_ft, y_test_bow_ft = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "X_train_bow_ft, X_val_bow_ft, y_train_bow_ft, y_val_bow_ft = train_test_split(X_train_bow_ft, y_train_bow_ft, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bow_ft.value_counts(normalize=True) #imbalance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598ab6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df8d7b0c",
   "metadata": {},
   "source": [
    "### **3.3.2 Word 2 Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc97d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = fake_true['text'].apply(lambda x: x.split())\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b658a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_ft = w2v_train_test_prepatarion(fake_true['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd25a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fake_true['label']\n",
    "X_train_w2v_ft, X_test_w2v_ft,y_train_w2v_ft, y_test_w2v_ft = train_test_split(X_w2v_ft, y, test_size=0.2, random_state=42)\n",
    "X_train_w2v_ft, X_val_w2v_ft,y_train_w2v_ft, y_val_w2v_ft = train_test_split(X_train_w2v_ft, y_train_w2v_ft, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_w2v_ft.value_counts(normalize=True) #imbalance check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3263189",
   "metadata": {},
   "source": [
    "## 4. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd6c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "#     else \"mps\"\n",
    "#     if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46744b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs =torch.sigmoid( self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, target) in tqdm(enumerate(train_loader), desc=\"Epoch %s: \" % (epoch+1), total=train_loader.__len__()):\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, target in valid_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                target = target.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), target)\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "                pred = (outputs > 0.5).float() \n",
    "                correct += (pred.squeeze() == target).float().sum().item()\n",
    "                \n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        valid_acc = correct / len(valid_loader.dataset)\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, loss.item(), valid_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfa807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(inputs)\n",
    "            pred = (outputs > 0.5).float() \n",
    "            predictions.append(pred.squeeze().cpu().numpy())\n",
    "    predictions = np.concatenate(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7bdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset): #created due to lack of memory\n",
    "    def __init__(self, X, y, use_toarray=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.use_toarray = use_toarray\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.use_toarray:\n",
    "            X_dense = torch.tensor(self.X[index]).float()\n",
    "        else:\n",
    "            X_dense = torch.tensor(self.X.iloc[index]).float()\n",
    "        y_dense = torch.tensor(self.y.iloc[index]).float()\n",
    "        return X_dense, y_dense\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "epochs_num=2\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d5226",
   "metadata": {},
   "source": [
    "## 4.1 WELFake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3fb1d8",
   "metadata": {},
   "source": [
    "### 4.1.1 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = LoadDataset(X_val_bow_wel, y_val_bow_wel)\n",
    "val_loader_wel= DataLoader(valid_loader, \n",
    "                           batch_size=16, \n",
    "                           shuffle=True)\n",
    "\n",
    "train_loader = LoadDataset(X_train_bow_wel, y_train_bow_wel)\n",
    "train_loader_wel= DataLoader(train_loader, \n",
    "                             batch_size=16, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0732441",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bow_wel = LogisticRegression(input_dim=X_train_bow_wel.shape[1], output_dim=1)\n",
    "model_bow_wel.to(device)\n",
    "optimizer = torch.optim.Adam(model_bow_wel.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d27ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train(model_bow_wel, train_loader_wel, val_loader_wel, criterion, optimizer, device, epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b561d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db463c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = LoadDataset(X_test_bow_wel, y_test_bow_wel)\n",
    "test_loader_wel= DataLoader(test_loader, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_wel = predict(model_bow_wel, test_loader_wel)\n",
    "\n",
    "print(classification_report(y_test_bow_wel, y_pred_wel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb105f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e9f71f",
   "metadata": {},
   "source": [
    "### 4.1.2 Word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = LoadDataset(X_val_w2v_wel, y_val_w2v_wel,use_toarray=True)\n",
    "val_loader_w2v_wel= DataLoader(valid_loader, \n",
    "                           batch_size=32, \n",
    "                           shuffle=True)\n",
    "\n",
    "train_loader = LoadDataset(X_train_w2v_wel, y_train_w2v_wel, use_toarray=True)\n",
    "train_loader_w2v_wel= DataLoader(train_loader, \n",
    "                                 batch_size=32, \n",
    "                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4367f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_wel = LogisticRegression(input_dim=100, output_dim=1)\n",
    "model_w2v_wel.to(device)\n",
    "optimizer = torch.optim.Adam(model_w2v_wel.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640a357",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(model_w2v_wel, train_loader_w2v_wel, val_loader_wel,  criterion, optimizer, device, epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a087975",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = LoadDataset(X_test_w2v_wel, y_test_w2v_wel, use_toarray=True)\n",
    "test_loader_w2v= DataLoader(test_loader, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_w2v = predict(model_w2v_wel, test_loader_w2v)\n",
    "\n",
    "print(classification_report(y_test_w2v_wel, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14bf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bafdd2c1",
   "metadata": {},
   "source": [
    "## 4.2 Fake_news_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b425d69",
   "metadata": {},
   "source": [
    "### 4.2.1 Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2987d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = LoadDataset(X_val_bow_net, y_val_bow_net)\n",
    "val_loader_net= DataLoader(valid_loader, \n",
    "                           batch_size=32, \n",
    "                           shuffle=True)\n",
    "\n",
    "train_loader = LoadDataset(X_train_bow_net, y_train_bow_net)\n",
    "train_loader_net= DataLoader(train_loader, \n",
    "                             batch_size=32, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd93b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bow_net = LogisticRegression(input_dim=X_train_bow_net.shape[1], output_dim=1)\n",
    "model_bow_net.to(device)\n",
    "optimizer = torch.optim.Adam(model_bow_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2564a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train(model_bow_net, train_loader_net, val_loader_net, criterion, optimizer, device, epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = LoadDataset(X_test_bow_net, y_test_bow_net)\n",
    "test_loader_net= DataLoader(test_loader, \n",
    "                            batch_size=32, \n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_net = predict(model_bow_net, test_loader_net)\n",
    "\n",
    "print(classification_report(y_test_bow_net, y_pred_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02109d6c",
   "metadata": {},
   "source": [
    "### 4.2.2 Word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafe5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = LoadDataset(X_val_w2v_net, y_val_w2v_net,use_toarray=True)\n",
    "val_loader_net= DataLoader(valid_loader, \n",
    "                           batch_size=32, \n",
    "                           shuffle=True)\n",
    "\n",
    "train_loader = LoadDataset(X_train_w2v_net, y_train_w2v_net,use_toarray=True)\n",
    "train_loader_net= DataLoader(train_loader, \n",
    "                             batch_size=32, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_net = LogisticRegression(input_dim=100, output_dim=1)\n",
    "model_w2v_net.to(device)\n",
    "optimizer = torch.optim.Adam(model_w2v_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(model_w2v_net, train_loader_net, val_loader_net, criterion, optimizer, device, epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = LoadDataset(X_test_w2v_net, y_test_w2v_net, use_toarray=True)\n",
    "test_loader_net= DataLoader(test_loader, \n",
    "                            batch_size=32, \n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae86c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_net = predict(model_w2v_net, test_loader_net)\n",
    "\n",
    "print(classification_report(y_test_w2v_net, y_pred_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e12a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0513453d",
   "metadata": {},
   "source": [
    "## 4.3 Fake True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fd1fe",
   "metadata": {},
   "source": [
    "### 4.3.1 Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275df804",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = LoadDataset(X_val_bow_ft, y_val_bow_ft)\n",
    "val_loader_ft= DataLoader(valid_loader, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True)\n",
    "\n",
    "train_loader = LoadDataset(X_train_bow_ft, y_train_bow_ft)\n",
    "train_loader_ft= DataLoader(train_loader, \n",
    "                            batch_size=32, \n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29158db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bow_ft = LogisticRegression(input_dim=X_train_bow_ft.shape[1], output_dim=1)\n",
    "model_bow_ft.to(device)\n",
    "optimizer = torch.optim.Adam(model_bow_ft.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(model_bow_ft, train_loader_ft, val_loader_ft, criterion, optimizer, device, epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a582a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = LoadDataset(X_test_bow_ft, y_test_bow_ft)\n",
    "test_loader_ft= DataLoader(test_loader, \n",
    "                            batch_size=32, \n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ft = predict(model_bow_ft, test_loader_ft)\n",
    "\n",
    "print(classification_report(y_test_bow_ft, y_pred_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd18ab9",
   "metadata": {},
   "source": [
    "### 4.3.2 Word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = LoadDataset(X_val_w2v_ft, y_val_w2v_ft,use_toarray=True)\n",
    "val_loader_ft= DataLoader(valid_loader, \n",
    "                           batch_size=32, \n",
    "                           shuffle=True)\n",
    "\n",
    "train_loader = LoadDataset(X_train_w2v_ft, y_train_w2v_ft,use_toarray=True)\n",
    "train_loader_ft= DataLoader(train_loader, \n",
    "                             batch_size=32, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_ft = LogisticRegression(input_dim=100, output_dim=1)\n",
    "model_w2v_ft.to(device)\n",
    "optimizer = torch.optim.Adam(model_w2v_ft.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42cd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(model_w2v_ft, train_loader_ft, val_loader_ft, criterion, optimizer, device, epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9207ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = LoadDataset(X_test_w2v_ft, y_test_w2v_ft, use_toarray=True)\n",
    "test_loader_ft= DataLoader(test_loader, \n",
    "                            batch_size=32, \n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ft = predict(model_w2v_ft, test_loader_ft)\n",
    "\n",
    "print(classification_report(y_test_w2v_ft, y_pred_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245baf6",
   "metadata": {},
   "source": [
    "## **5. Hyperparameters tunning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-3, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-3, 1e-1)\n",
    "\n",
    "\n",
    "    model = LogisticRegression(input_dim=X, output_dim=1)\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            imputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels).float().sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(study, trial):\n",
    "    print(\"Trial finished with value: \", trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde12985",
   "metadata": {},
   "source": [
    "### 5.1 WELFake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae07daff",
   "metadata": {},
   "source": [
    "### 5.1.1 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92257d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_bow_wel.shape[1]\n",
    "train_loader = train_loader= train_loader_wel\n",
    "val_loader = val_loader_wel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print('Best hyperparameters:', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73b064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f226953",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 100\n",
    "train_loader = train_loader= train_loader_w2v_wel\n",
    "val_loader = val_loader_w2v_wel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10, callbacks=[callback])\n",
    "\n",
    "print('Best hyperparameters:', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453928a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (PyTorch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
